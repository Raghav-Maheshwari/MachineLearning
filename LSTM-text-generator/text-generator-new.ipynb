{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tf/lib/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load Larger LSTM network and generate text\n",
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"data/input.txt\"\n",
    "raw_text = open(filename, encoding=\"utf8\").read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  62386\n",
      "Total Vocab:  65\n"
     ]
    }
   ],
   "source": [
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  62286\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a larger network:\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights/weights-actual-new-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.3118\n",
      "Epoch 00001: loss improved from 1.32599 to 1.31172, saving model to weights/weights-new-improvement-01-1.3117-bigger.hdf5\n",
      "62286/62286 [==============================] - 126s 2ms/step - loss: 1.3117\n",
      "Epoch 2/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.3075\n",
      "Epoch 00002: loss improved from 1.31172 to 1.30752, saving model to weights/weights-new-improvement-02-1.3075-bigger.hdf5\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.3075\n",
      "Epoch 3/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.2974\n",
      "Epoch 00003: loss improved from 1.30752 to 1.29763, saving model to weights/weights-new-improvement-03-1.2976-bigger.hdf5\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.2976\n",
      "Epoch 4/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.2863\n",
      "Epoch 00004: loss improved from 1.29763 to 1.28621, saving model to weights/weights-new-improvement-04-1.2862-bigger.hdf5\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.2862\n",
      "Epoch 5/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.2842\n",
      "Epoch 00005: loss improved from 1.28621 to 1.28414, saving model to weights/weights-new-improvement-05-1.2841-bigger.hdf5\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.2841\n",
      "Epoch 6/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.2713\n",
      "Epoch 00006: loss improved from 1.28414 to 1.27129, saving model to weights/weights-new-improvement-06-1.2713-bigger.hdf5\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.2713\n",
      "Epoch 7/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.2598\n",
      "Epoch 00007: loss improved from 1.27129 to 1.25983, saving model to weights/weights-new-improvement-07-1.2598-bigger.hdf5\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.2598\n",
      "Epoch 8/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.2548\n",
      "Epoch 00008: loss improved from 1.25983 to 1.25481, saving model to weights/weights-new-improvement-08-1.2548-bigger.hdf5\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.2548\n",
      "Epoch 9/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.2540\n",
      "Epoch 00009: loss improved from 1.25481 to 1.25394, saving model to weights/weights-new-improvement-09-1.2539-bigger.hdf5\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.2539\n",
      "Epoch 10/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.2550\n",
      "Epoch 00010: loss did not improve\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.2552\n",
      "Epoch 11/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.2468\n",
      "Epoch 00011: loss improved from 1.25394 to 1.24683, saving model to weights/weights-new-improvement-11-1.2468-bigger.hdf5\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.2468\n",
      "Epoch 12/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.2411\n",
      "Epoch 00012: loss improved from 1.24683 to 1.24114, saving model to weights/weights-new-improvement-12-1.2411-bigger.hdf5\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.2411\n",
      "Epoch 13/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.2295\n",
      "Epoch 00013: loss improved from 1.24114 to 1.22938, saving model to weights/weights-new-improvement-13-1.2294-bigger.hdf5\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.2294\n",
      "Epoch 14/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.2161\n",
      "Epoch 00014: loss improved from 1.22938 to 1.21607, saving model to weights/weights-new-improvement-14-1.2161-bigger.hdf5\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.2161\n",
      "Epoch 15/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.2194\n",
      "Epoch 00015: loss did not improve\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.2195\n",
      "Epoch 16/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.2100\n",
      "Epoch 00016: loss improved from 1.21607 to 1.21008, saving model to weights/weights-new-improvement-16-1.2101-bigger.hdf5\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.2101\n",
      "Epoch 17/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.2153\n",
      "Epoch 00017: loss did not improve\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.2153\n",
      "Epoch 18/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.1969\n",
      "Epoch 00018: loss improved from 1.21008 to 1.19682, saving model to weights/weights-new-improvement-18-1.1968-bigger.hdf5\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.1968\n",
      "Epoch 19/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.2032\n",
      "Epoch 00019: loss did not improve\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.2032\n",
      "Epoch 20/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.2041\n",
      "Epoch 00020: loss did not improve\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.2040\n",
      "Epoch 21/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.1976\n",
      "Epoch 00021: loss did not improve\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.1976\n",
      "Epoch 22/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.1841\n",
      "Epoch 00022: loss improved from 1.19682 to 1.18417, saving model to weights/weights-new-improvement-22-1.1842-bigger.hdf5\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.1842\n",
      "Epoch 23/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.1779\n",
      "Epoch 00023: loss improved from 1.18417 to 1.17779, saving model to weights/weights-new-improvement-23-1.1778-bigger.hdf5\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.1778\n",
      "Epoch 24/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.1805\n",
      "Epoch 00024: loss did not improve\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.1805\n",
      "Epoch 25/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.1845\n",
      "Epoch 00025: loss did not improve\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.1845\n",
      "Epoch 26/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.1686\n",
      "Epoch 00026: loss improved from 1.17779 to 1.16863, saving model to weights/weights-new-improvement-26-1.1686-bigger.hdf5\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.1686\n",
      "Epoch 27/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.1720\n",
      "Epoch 00027: loss did not improve\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.1720\n",
      "Epoch 28/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.1665\n",
      "Epoch 00028: loss improved from 1.16863 to 1.16645, saving model to weights/weights-new-improvement-28-1.1665-bigger.hdf5\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.1665\n",
      "Epoch 29/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.1612\n",
      "Epoch 00029: loss improved from 1.16645 to 1.16120, saving model to weights/weights-new-improvement-29-1.1612-bigger.hdf5\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.1612\n",
      "Epoch 30/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.1674\n",
      "Epoch 00030: loss did not improve\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.1675\n",
      "Epoch 31/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.1701\n",
      "Epoch 00031: loss did not improve\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.1702\n",
      "Epoch 32/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.1615\n",
      "Epoch 00032: loss did not improve\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.1615\n",
      "Epoch 33/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.1539\n",
      "Epoch 00033: loss improved from 1.16120 to 1.15391, saving model to weights/weights-new-improvement-33-1.1539-bigger.hdf5\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.1539\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.1633\n",
      "Epoch 00034: loss did not improve\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.1634\n",
      "Epoch 35/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.1706\n",
      "Epoch 00035: loss did not improve\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.1707\n",
      "Epoch 36/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.1455\n",
      "Epoch 00036: loss improved from 1.15391 to 1.14537, saving model to weights/weights-new-improvement-36-1.1454-bigger.hdf5\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.1454\n",
      "Epoch 37/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.1553\n",
      "Epoch 00037: loss did not improve\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.1554\n",
      "Epoch 38/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.1419\n",
      "Epoch 00038: loss improved from 1.14537 to 1.14177, saving model to weights/weights-new-improvement-38-1.1418-bigger.hdf5\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.1418\n",
      "Epoch 39/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.1360\n",
      "Epoch 00039: loss improved from 1.14177 to 1.13595, saving model to weights/weights-new-improvement-39-1.1360-bigger.hdf5\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.1360\n",
      "Epoch 40/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.1766\n",
      "Epoch 00040: loss did not improve\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.1765\n",
      "Epoch 41/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 2.3835\n",
      "Epoch 00041: loss did not improve\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 2.3848\n",
      "Epoch 42/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 4.2789\n",
      "Epoch 00042: loss did not improve\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 4.2787\n",
      "Epoch 43/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 3.1509\n",
      "Epoch 00043: loss did not improve\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 3.1508\n",
      "Epoch 44/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 2.9349\n",
      "Epoch 00044: loss did not improve\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 2.9351\n",
      "Epoch 45/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 2.8551\n",
      "Epoch 00045: loss did not improve\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 2.8551\n",
      "Epoch 46/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 2.7760\n",
      "Epoch 00046: loss did not improve\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 2.7759\n",
      "Epoch 47/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 2.6323\n",
      "Epoch 00047: loss did not improve\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 2.6324\n",
      "Epoch 48/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 2.5150\n",
      "Epoch 00048: loss did not improve\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 2.5150\n",
      "Epoch 49/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 2.3160\n",
      "Epoch 00049: loss did not improve\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 2.3161\n",
      "Epoch 50/50\n",
      "62272/62286 [============================>.] - ETA: 0s - loss: 1.8320\n",
      "Epoch 00050: loss did not improve\n",
      "62286/62286 [==============================] - 127s 2ms/step - loss: 1.8319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6700a80198>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights/weights-new-improvement-39-1.1360-bigger.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" price continued dropping, and stabilized at $200. the word bitcoin began to take on a negative conno \"\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titise are seares and ueryire a manareted in the future. the una orobucts are stilled uwstem bnd cuaiteng that means is the euture. the inperemt bry fown the eeat their teemitely become bdsivi incges of toral and inowlad seatets and secure indrstry. the dreative insestment petiercsuar hon swstem she rrivia in many eoause in the future. the inporsutity to have a poool of veakling of a great ford in the future. the inporsutity to the toars contracts to the rralicnt cri the dompletity of the eraatiog of venah from and henerald so the real probets of any fueat surpess in yhich cou these is so coeas the torst of the fattl seaton â€“ all the coonary teatoi of the eraatiog of semoens and iemeer dremples and inweraction iase the transactions coml tr thre antthing that welle to tafeno of the forele is a master of luck lore semeas hore that the pean pore of the beteriti sooply of oveer bre cara sodotaru and supply cating ther welle boologoi and henerald megh would be able to do the olpy detini in the future. the und of tokenization is a periou lroblem retoacie secenre to to fond and hoo your domled a dompumtu of the exahange of the ciaie with tokenization that will have to co a couple of years and how that the pead more about what the price agdins for the more thmel bu the oayment of your products and secure industry. it was dreatid you can start blncks from make and that the tese in this was sevory of your products are not al impights ald secorisy tecmgniz are it ceense their evrraation tessinn in the ooopitu of a gog bnd domsud thes well to the puotem and the dreateve investment clals the transactions complex racogdel probebly rewersed dy transfor secomnents and srars po the rlaytic litws of ins an inpighti computins in the hialt in the fiait form on the fore that your fene im the fomsotu artificial intelligence trevtariny teat beter but the iame uo an ontonline the eate aceomit in the figital currency addisset fandims to be ar insented and probrees tuing citcoin is the futur\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# generate characters\n",
    "for i in range(2000):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
